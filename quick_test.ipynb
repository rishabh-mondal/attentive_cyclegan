{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 01:16:08.715026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-13 01:16:08.738470: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-13 01:16:08.738547: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 01:16:08.753359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-13 01:16:09.763572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "print('running')\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from itertools import zip_longest\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ImageDataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.file_paths = [\n",
    "            os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path)\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Open the image and convert to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if provided\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define transformation pipeline\n",
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to Tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_path):\n",
    "    dataset = ImageDataset(folder_path, transform=transform_pipeline)\n",
    "    # Added num_workers and pin_memory for better performance\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4,  # Adjust this based on your CPU cores\n",
    "        pin_memory=True  # Speeds up data transfer to GPU if using CUDA\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = load_images('/home/umang.shikarvar/distance_exp/west_bengal_same_class_count_10_120_1000/images')\n",
    "target = load_images('/home/umang.shikarvar/distance_exp/haryana_same_class_count_10_120_1000/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_downsampling=True, add_activation=True, **kwargs):\n",
    "        super().__init__()\n",
    "        if is_downsampling:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvolutionalBlock(channels, channels, add_activation=True, kernel_size=3, padding=1),\n",
    "            ConvolutionalBlock(channels, channels, add_activation=False, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention Module\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, C, H, W = x.shape\n",
    "        query = self.query(x).view(batch, -1, W * H).permute(0, 2, 1)\n",
    "        key = self.key(x).view(batch, -1, W * H)\n",
    "        attention = self.softmax(torch.bmm(query, key))\n",
    "        value = self.value(x).view(batch, -1, W * H)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1)).view(batch, C, H, W)\n",
    "        return x + out  # Residual connection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding SE Block (Channel Attention)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, C, _, _ = x.size()\n",
    "        y = self.global_avg_pool(x).view(batch, C)\n",
    "        y = self.fc(y).view(batch, C, 1, 1)\n",
    "        return x * y.expand_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Convolution Layers to Use Spectral Normalization\n",
    "\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "\n",
    "class SpectralConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(SpectralConv, self).__init__()\n",
    "        self.conv = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode=\"reflect\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=6):\n",
    "        super().__init__()\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.downsampling_layers = nn.ModuleList([\n",
    "            ConvolutionalBlock(num_features, num_features * 2, is_downsampling=True, kernel_size=3, stride=2, padding=1),\n",
    "            ConvolutionalBlock(num_features * 2, num_features * 4, is_downsampling=True, kernel_size=3, stride=2, padding=1),\n",
    "        ])\n",
    "\n",
    "        self.attention1 = SelfAttention(num_features * 4)  # Add Self-Attention at bottleneck\n",
    "\n",
    "        self.residual_layers = nn.Sequential(*[ResidualBlock(num_features * 4) for _ in range(num_residuals)])\n",
    "\n",
    "        self.channel_attention = SEBlock(num_features * 4)  # Apply SE Block after residual layers\n",
    "\n",
    "        self.upsampling_layers = nn.ModuleList([\n",
    "            ConvolutionalBlock(num_features * 4, num_features * 2, is_downsampling=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ConvolutionalBlock(num_features * 2, num_features * 1, is_downsampling=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        ])\n",
    "\n",
    "        self.last_layer = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        for layer in self.downsampling_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.attention1(x)  # Self-Attention\n",
    "\n",
    "        x = self.residual_layers(x)\n",
    "\n",
    "        x = self.channel_attention(x)  # SE Block for channel attention\n",
    "\n",
    "        for layer in self.upsampling_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return torch.tanh(self.last_layer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def denoise_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    denoised = cv2.bilateralFilter(image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    return denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_edges(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    enhanced = cv2.addWeighted(image, 1.5, laplacian, -0.5, 0)\n",
    "    return enhanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\")),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                spectral_norm(nn.Conv2d(in_channels, feature, kernel_size=4, stride=2 if feature != features[-1] else 1, padding=1, padding_mode=\"reflect\"))\n",
    "            )\n",
    "            layers.append(nn.InstanceNorm2d(feature))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "            # Add Self-Attention after 256 channels\n",
    "            if feature == 256:\n",
    "                layers.append(SelfAttention(feature))\n",
    "\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(spectral_norm(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator_g = Generator(img_channels=3).to(device)  # G: X → Y\n",
    "generator_f = Generator(img_channels=3).to(device)  # F: Y → X\n",
    "discriminator_x = Discriminator().to(device)  # Discriminator for domain X\n",
    "discriminator_y = Discriminator().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n",
      "Generator G (X → Y): 7,928,403\n",
      "Generator F (Y → X): 7,928,403\n",
      "Discriminator X: 2,846,977\n",
      "Discriminator Y: 2,846,977\n",
      "Total Trainable Parameters: 21,550,760\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count parameters for each model\n",
    "g_g_params = count_parameters(generator_g)\n",
    "g_f_params = count_parameters(generator_f)\n",
    "d_x_params = count_parameters(discriminator_x)\n",
    "d_y_params = count_parameters(discriminator_y)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Trainable Parameters:\")\n",
    "print(f\"Generator G (X → Y): {g_g_params:,}\")\n",
    "print(f\"Generator F (Y → X): {g_f_params:,}\")\n",
    "print(f\"Discriminator X: {d_x_params:,}\")\n",
    "print(f\"Discriminator Y: {d_y_params:,}\")\n",
    "\n",
    "# Total trainable parameters\n",
    "total_params = g_g_params + g_f_params + d_x_params + d_y_params\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.88 GiB. GPU 3 has a total capacity of 79.25 GiB of which 4.33 GiB is free. Including non-PyTorch memory, this process has 74.91 GiB memory in use. Of the allocated memory 72.12 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Cycle-consistency loss\u001b[39;00m\n\u001b[1;32m     66\u001b[0m cycle_x \u001b[38;5;241m=\u001b[39m generator_f(fake_y)  \u001b[38;5;66;03m# F(G(X)) ≈ X\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m cycle_y \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator_g\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_x\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# G(F(Y)) ≈ Y\u001b[39;00m\n\u001b[1;32m     68\u001b[0m cycle_loss_x \u001b[38;5;241m=\u001b[39m cycle_consistency_loss(real_x, cycle_x)\n\u001b[1;32m     69\u001b[0m cycle_loss_y \u001b[38;5;241m=\u001b[39m cycle_consistency_loss(real_y, cycle_y)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsampling_layers:\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_layers(x)\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_attention(x)  \u001b[38;5;66;03m# SE Block for channel attention\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x)\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, W \u001b[38;5;241m*\u001b[39m H)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x)\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, W \u001b[38;5;241m*\u001b[39m H)\n\u001b[0;32m---> 15\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, W \u001b[38;5;241m*\u001b[39m H)\n\u001b[1;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(value, attention\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(batch, C, H, W)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.88 GiB. GPU 3 has a total capacity of 79.25 GiB of which 4.33 GiB is free. Including non-PyTorch memory, this process has 74.91 GiB memory in use. Of the allocated memory 72.12 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Instantiate models\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " # Discriminator for domain Y\n",
    "\n",
    "# Optimizers\n",
    "lr = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "generator_g_optimizer = optim.Adam(generator_g.parameters(), lr=lr, betas=betas)\n",
    "generator_f_optimizer = optim.Adam(generator_f.parameters(), lr=lr, betas=betas)\n",
    "discriminator_x_optimizer = optim.Adam(discriminator_x.parameters(), lr=lr, betas=betas)\n",
    "discriminator_y_optimizer = optim.Adam(discriminator_y.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "# Learning rate schedulers\n",
    "def lr_lambda(epoch):\n",
    "    return 1.0 - max(0, epoch - 100) / 100  # Linear decay after 100 epochs\n",
    "\n",
    "scheduler_g = optim.lr_scheduler.LambdaLR(generator_g_optimizer, lr_lambda=lr_lambda)\n",
    "scheduler_f = optim.lr_scheduler.LambdaLR(generator_f_optimizer, lr_lambda=lr_lambda)\n",
    "scheduler_dx = optim.lr_scheduler.LambdaLR(discriminator_x_optimizer, lr_lambda=lr_lambda)\n",
    "scheduler_dy = optim.lr_scheduler.LambdaLR(discriminator_y_optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "def cycle_consistency_loss(real, cycled):\n",
    "    return l1_loss(real, cycled) * 10.0  # Weight factor 10.0\n",
    "\n",
    "def identity_loss(real, same):\n",
    "    return l1_loss(real, same) * 5.0  # Weight factor 5.0\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='tb_logs')\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 200  # Define total epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    g_loss_total, f_loss_total, dx_loss_total, dy_loss_total = 0, 0, 0, 0\n",
    "    cycle_loss_total, identity_loss_total = 0, 0\n",
    "\n",
    "    for real_x_batch, real_y_batch in zip_longest(source, target, fillvalue=None):\n",
    "        if real_x_batch is None or real_y_batch is None:\n",
    "            continue\n",
    "\n",
    "        real_x, real_y = real_x_batch.to(device), real_y_batch.to(device)\n",
    "\n",
    "        # ------------------------\n",
    "        # Train Generators G and F\n",
    "        # ------------------------\n",
    "\n",
    "        # Identity loss (G(Y) ≈ Y and F(X) ≈ X)\n",
    "        identity_x = generator_f(real_x)\n",
    "        identity_y = generator_g(real_y)\n",
    "        id_loss_x = identity_loss(real_x, identity_x)\n",
    "        id_loss_y = identity_loss(real_y, identity_y)\n",
    "\n",
    "        # Adversarial loss\n",
    "        fake_y = generator_g(real_x)  # G(X)\n",
    "        fake_x = generator_f(real_y)  # F(Y)\n",
    "\n",
    "        adv_loss_g = mse_loss(discriminator_y(fake_y), torch.ones_like(discriminator_y(fake_y)))\n",
    "        adv_loss_f = mse_loss(discriminator_x(fake_x), torch.ones_like(discriminator_x(fake_x)))\n",
    "\n",
    "        # Cycle-consistency loss\n",
    "        cycle_x = generator_f(fake_y)  # F(G(X)) ≈ X\n",
    "        cycle_y = generator_g(fake_x)  # G(F(Y)) ≈ Y\n",
    "        cycle_loss_x = cycle_consistency_loss(real_x, cycle_x)\n",
    "        cycle_loss_y = cycle_consistency_loss(real_y, cycle_y)\n",
    "\n",
    "        # Total generator loss\n",
    "        total_g_loss = adv_loss_g + cycle_loss_x + id_loss_y\n",
    "        total_f_loss = adv_loss_f + cycle_loss_y + id_loss_x\n",
    "\n",
    "        generator_g_optimizer.zero_grad()\n",
    "        generator_f_optimizer.zero_grad()\n",
    "        total_g_loss.backward(retain_graph=True)\n",
    "        total_f_loss.backward()\n",
    "        generator_g_optimizer.step()\n",
    "        generator_f_optimizer.step()\n",
    "\n",
    "        # -------------------------\n",
    "        # Train Discriminators X, Y\n",
    "        # -------------------------\n",
    "\n",
    "        # Discriminator X loss\n",
    "        real_loss_x = mse_loss(discriminator_x(real_x), torch.ones_like(discriminator_x(real_x)))\n",
    "        fake_loss_x = mse_loss(discriminator_x(fake_x.detach()), torch.zeros_like(discriminator_x(fake_x)))\n",
    "        dx_loss = (real_loss_x + fake_loss_x) * 0.5\n",
    "\n",
    "        discriminator_x_optimizer.zero_grad()\n",
    "        dx_loss.backward()\n",
    "        discriminator_x_optimizer.step()\n",
    "\n",
    "        # Discriminator Y loss\n",
    "        real_loss_y = mse_loss(discriminator_y(real_y), torch.ones_like(discriminator_y(real_y)))\n",
    "        fake_loss_y = mse_loss(discriminator_y(fake_y.detach()), torch.zeros_like(discriminator_y(fake_y)))\n",
    "        dy_loss = (real_loss_y + fake_loss_y) * 0.5\n",
    "\n",
    "        discriminator_y_optimizer.zero_grad()\n",
    "        dy_loss.backward()\n",
    "        discriminator_y_optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        g_loss_total += total_g_loss.item()\n",
    "        f_loss_total += total_f_loss.item()\n",
    "        dx_loss_total += dx_loss.item()\n",
    "        dy_loss_total += dy_loss.item()\n",
    "        cycle_loss_total += cycle_loss_x.item() + cycle_loss_y.item()\n",
    "        identity_loss_total += id_loss_x.item() + id_loss_y.item()\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler_g.step()\n",
    "    scheduler_f.step()\n",
    "    scheduler_dx.step()\n",
    "    scheduler_dy.step()\n",
    "\n",
    "    # Log losses to TensorBoard\n",
    "    writer.add_scalar('Loss/Generator_G', g_loss_total, epoch + 1)\n",
    "    writer.add_scalar('Loss/Generator_F', f_loss_total, epoch + 1)\n",
    "    writer.add_scalar('Loss/Discriminator_X', dx_loss_total, epoch + 1)\n",
    "    writer.add_scalar('Loss/Discriminator_Y', dy_loss_total, epoch + 1)\n",
    "    writer.add_scalar('Loss/Cycle_Consistency', cycle_loss_total, epoch + 1)\n",
    "    writer.add_scalar('Loss/Identity', identity_loss_total, epoch + 1)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{EPOCHS}]: \"\n",
    "        f\"G_loss: {g_loss_total:.4f}, F_loss: {f_loss_total:.4f}, \"\n",
    "        f\"D_X_loss: {dx_loss_total:.4f}, D_Y_loss: {dy_loss_total:.4f}, \"\n",
    "        f\"Cycle_loss: {cycle_loss_total:.4f}, Identity_loss: {identity_loss_total:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save model checkpoints every 10 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        torch.save(generator_g.state_dict(), f'generator_WB_to_Haryana_{epoch+1}.pth')\n",
    "        torch.save(generator_f.state_dict(), f'generator_Haryana_to_WB_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCCL Backend Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "print(f\"NCCL Backend Available: {dist.is_nccl_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using free port: 50175\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "def find_free_port():\n",
    "    \"\"\"\n",
    "    Finds an available port on the system dynamically.\n",
    "    Returns:\n",
    "        int: Available port number.\n",
    "    \"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\", 0))  # Bind to an available port\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "# Example Usage\n",
    "free_port = find_free_port()\n",
    "print(f\"Using free port: {free_port}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
